# RKNN多线程推理问题分析报告

## 概述

本报告总结了在实现RKNN线程池多核推理过程中遇到的问题，包括问题现象、原因分析和解决方案。

---

## 问题一：Double Free (双重释放)

### 问题现象

```bash
root@RK3588:~/rknn_model# ./rknn_model pool5 ./model/car.jpg
[info]Got result 1: detected 37 objects
...
[info]Got result 9: detected 37 objects
double free or corruption (out)
Aborted
```

程序在运行一段时间后崩溃，报错 `double free or corruption`。

### 原因分析

**成员变量未初始化**导致析构时释放野指针。

在C++中，类的成员变量如果是指针类型且未显式初始化，其值是**未定义的**（可能是任意值）。当析构函数尝试释放这些"野指针"时，就会导致双重释放错误。

#### 问题代码

```cpp
// rknn_model.cc - 构造函数（问题版本）
rknn::Model::Model(std::string model_path, logger::Level level) {
    m_rknnPath = model_path;
    m_logger = std::make_shared<logger::Logger>(level);
    m_params = std::make_unique<Params>();
    // 问题：m_inputAttrs, m_outputAttrs, m_rknnCtx 未初始化！
    init_model(nullptr);
}

// 析构函数
rknn::Model::~Model() {
    if(m_inputAttrs != NULL){      // m_inputAttrs 可能是野指针，判断无效
        free(m_inputAttrs);         // 释放野指针 -> 崩溃！
        m_inputAttrs = NULL;
    }
    if(m_outputAttrs != NULL){
        free(m_outputAttrs);
        m_outputAttrs = NULL;
    }
    if(m_rknnCtx != 0){
        rknn_destroy(m_rknnCtx);
        m_rknnCtx = 0;
    }
}
```

#### 内存状态示意

```
未初始化时：
┌─────────────────┬─────────────────────────────┐
│ m_inputAttrs    │ 0x7fff12345678 (随机值)     │ <- 野指针！
├─────────────────┼─────────────────────────────┤
│ m_outputAttrs   │ 0x00007abc1234 (随机值)     │ <- 野指针！
├─────────────────┼─────────────────────────────┤
│ m_rknnCtx       │ 123456789 (随机值)          │ <- 无效context！
└─────────────────┴─────────────────────────────┘
```

### 解决方案

**在构造函数中显式初始化所有指针成员为 `nullptr` 或 `0`**

```cpp
// rknn_model.cc - 构造函数（修复版本）
rknn::Model::Model(std::string model_path, logger::Level level) {
    m_rknnPath = model_path;
    m_logger = std::make_shared<logger::Logger>(level);
    m_params = std::make_unique<Params>();

    // 关键修复：显式初始化所有指针
    m_inputAttrs = nullptr;
    m_outputAttrs = nullptr;
    m_rknnCtx = 0;

    init_model(nullptr);
}

rknn::Model::Model(std::string model_path, logger::Level level, rknn_context* ctx_in) {
    m_rknnPath = model_path;
    m_logger = std::make_shared<logger::Logger>(level);
    m_params = std::make_unique<Params>();

    // 关键修复：显式初始化所有指针
    m_inputAttrs = nullptr;
    m_outputAttrs = nullptr;
    m_rknnCtx = 0;

    init_model(ctx_in);
}
```

### 经验总结

| 规则 | 说明 |
|------|------|
| **始终初始化指针** | 在构造函数中将所有指针初始化为 `nullptr` |
| **使用智能指针** | 优先使用 `std::unique_ptr`/`std::shared_ptr` 代替裸指针 |
| **RAII原则** | 资源获取即初始化，确保资源生命周期可控 |

---

## 问题二：Segmentation Fault (段错误)

### 问题现象

```bash
root@RK3588:~/rknn_model# ./rknn_model pool5 ./model/car.jpg
[info]Got result 1: detected 37 objects
[info]Got result 2: detected 37 objects
[info]Got result 3: detected 37 objects
Segmentation fault
```

程序随机崩溃，出现段错误。

### 原因分析

**多线程竞争条件（Race Condition）**：多个线程同时访问同一个模型实例的成员变量。

#### 问题代码

```cpp
// rknn_model.cc - inference函数（问题版本）
rknn::ModelResult rknn::Model::inference(cv::Mat img) {
    int ret;
    m_img = img.clone();  // 线程A和线程B可能同时写入m_img！

    // 多个线程同时操作这些成员变量
    m_rknnInputPtr = std::make_unique<rknn_input[]>(m_ioNum.n_input);
    memset(m_rknnInputPtr.get(), 0, m_ioNum.n_input * sizeof(rknn_input));

    preprocess();  // 使用成员变量

    ret = rknn_inputs_set(m_rknnCtx, m_ioNum.n_input, m_rknnInputPtr.get());
    ret = rknn_run(m_rknnCtx, nullptr);

    m_rknnOutputPtr = std::make_unique<rknn_output[]>(m_ioNum.n_output);
    // ...

    postprocess();  // 使用成员变量

    return m_result;  // 返回成员变量
}
```

#### 竞争条件示意

```
时间线 →

线程A (Model 0):  m_img = imgA  ──────→  preprocess()  ──────→  m_result = resA
                       ↓                      ↓                      ↓
线程B (Model 0):       m_img = imgB  ──→  preprocess()  ──→  m_result = resB
                           ↑                  ↑                  ↑
                      数据被覆盖！         处理错误数据！      结果被覆盖！
```

当线程池将多个任务分配给同一个模型实例时，如果该模型实例正在处理另一个任务，就会发生数据竞争。

### 解决方案

**为每个模型实例添加互斥锁保护**

```cpp
// rknn_model.hpp - 添加互斥锁成员
class Model {
protected:
    // ... 其他成员 ...

    // 关键修复：添加互斥锁保护
    std::mutex m_inferenceMtx;
};

// rknn_model.cc - inference函数（修复版本）
rknn::ModelResult rknn::Model::inference(cv::Mat img) {
    // 关键修复：在函数入口加锁，确保同一时间只有一个线程访问此模型实例
    std::lock_guard<std::mutex> lock(m_inferenceMtx);

    int ret;
    m_img = img.clone();

    m_rknnInputPtr = std::make_unique<rknn_input[]>(m_ioNum.n_input);
    // ... 后续代码不变 ...

    return m_result;
}  // 函数结束时自动释放锁
```

#### 修复后的执行流程

```
时间线 →

线程A (Model 0):  [获取锁] → m_img = imgA → preprocess() → m_result = resA → [释放锁]
                                                                                    ↓
线程B (Model 0):  [等待锁..............................]  → [获取锁] → 正常执行 → [释放锁]
```

### 经验总结

| 规则 | 说明 |
|------|------|
| **识别共享资源** | 找出所有可能被多线程访问的成员变量 |
| **使用互斥锁** | 对共享资源的访问加锁保护 |
| **最小化锁范围** | 只锁定必要的代码段，避免性能损失 |
| **使用RAII锁** | 使用 `std::lock_guard` 自动管理锁的生命周期 |

---

## 问题三：线程池锁设计问题

### 问题现象

程序运行时偶尔卡死或出现死锁。

### 原因分析

**嵌套锁导致潜在死锁** 和 **阻塞get()操作导致无法提交新任务**

#### 问题代码

```cpp
// RknnPool.hpp（问题版本）

// 问题1：嵌套锁
int RknnPool::put(inputType inputData) {
    std::lock_guard<std::mutex> lock(m_queueMtx);  // 获取 m_queueMtx
    int modelId = getModelId();  // getModelId() 内部获取 m_idMtx，虽然不是同一个锁，但设计不清晰
    m_futures.push(m_pool->submit(&rknnModel::infer, m_models[modelId], inputData));
    return 0;
}

// 问题2：get()持有锁等待future
int RknnPool::get(outputType& outputData) {
    std::lock_guard<std::mutex> lock(m_queueMtx);  // 获取锁
    if (m_futures.empty())
        return 1;

    outputData = m_futures.front().get();  // 在持有锁的情况下等待future！
    m_futures.pop();                        // 其他线程无法调用put()或get()
    return 0;
}
```

#### 问题示意

```
线程A (消费者):  get() → [获取m_queueMtx] → future.get() → [等待中...锁未释放]
                                                              ↓
线程B (生产者):  put() → [等待m_queueMtx...] → 永远等待！
```

### 解决方案

**1. 将获取modelId移到锁外面**

```cpp
int RknnPool::put(inputType inputData) {
    int modelId = getModelId();  // 先获取modelId（使用m_idMtx）
    std::lock_guard<std::mutex> lock(m_queueMtx);  // 再获取队列锁
    m_futures.push(m_pool->submit(&rknnModel::infer, m_models[modelId], inputData));
    return 0;
}
```

**2. 在等待future前释放锁**

```cpp
int RknnPool::get(outputType& outputData) {
    std::unique_lock<std::mutex> lock(m_queueMtx);  // 使用unique_lock，支持手动unlock
    if (m_futures.empty())
        return 1;

    auto fut = std::move(m_futures.front());  // 移动future到局部变量
    m_futures.pop();
    lock.unlock();  // 关键修复：在等待前释放锁！

    outputData = fut.get();  // 现在可以安全等待，其他线程可以操作队列
    return 0;
}
```

**3. 修复析构函数**

```cpp
RknnPool::~RknnPool() {
    while (true) {
        std::unique_lock<std::mutex> lock(m_queueMtx);
        if (m_futures.empty())
            break;
        auto fut = std::move(m_futures.front());
        m_futures.pop();
        lock.unlock();  // 释放锁后再等待
        fut.get();
    }
    std::cout << "[RknnPool] Pool destroyed" << std::endl;
}
```

#### 修复后的执行流程

```
线程A (消费者):  get() → [获取锁] → 取出future → [释放锁] → future.get()
                                                      ↓
线程B (生产者):  put() → [获取锁] → 提交任务 → [释放锁]  ← 可以并行执行！
```

### 经验总结

| 规则 | 说明 |
|------|------|
| **避免持锁等待** | 不要在持有锁的情况下进行阻塞操作 |
| **使用unique_lock** | 当需要手动控制锁的释放时，使用 `std::unique_lock` |
| **锁粒度最小化** | 只在必要时持有锁，尽快释放 |
| **注意锁的顺序** | 多个锁时保持一致的获取顺序，避免死锁 |

---

## 总结：多线程编程检查清单

### 内存安全

- [ ] 所有指针成员在构造函数中初始化为 `nullptr`
- [ ] 析构函数检查指针有效性后再释放
- [ ] 优先使用智能指针管理动态内存

### 线程安全

- [ ] 识别所有共享资源（成员变量）
- [ ] 为共享资源添加互斥锁保护
- [ ] 使用 `std::lock_guard` 或 `std::unique_lock` 管理锁
- [ ] 避免在持有锁时进行阻塞操作

### 锁设计

- [ ] 避免嵌套锁（如必须，保持一致的获取顺序）
- [ ] 锁粒度最小化，只保护必要的代码
- [ ] 使用 `std::unique_lock` 当需要提前释放锁时

### 调试技巧

- [ ] 使用 `gdb` 调试段错误，查看崩溃时的调用栈
- [ ] 使用 `valgrind` 检测内存泄漏和非法访问
- [ ] 使用 `ThreadSanitizer` 检测数据竞争
- [ ] 在关键位置添加日志，追踪执行流程

---

## 附录：完整修复代码对比

### rknn_model.hpp

```cpp
// 添加互斥锁成员
class Model {
protected:
    // ... 其他成员 ...

    // 新增：线程安全互斥锁
    std::mutex m_inferenceMtx;
};
```

### rknn_model.cc

```cpp
// 构造函数 - 初始化指针
rknn::Model::Model(std::string model_path, logger::Level level) {
    m_rknnPath = model_path;
    m_logger = std::make_shared<logger::Logger>(level);
    m_params = std::make_unique<Params>();
    m_inputAttrs = nullptr;   // 新增
    m_outputAttrs = nullptr;  // 新增
    m_rknnCtx = 0;            // 新增
    init_model(nullptr);
}

// inference函数 - 添加锁保护
rknn::ModelResult rknn::Model::inference(cv::Mat img) {
    std::lock_guard<std::mutex> lock(m_inferenceMtx);  // 新增
    // ... 其余代码不变 ...
}
```

### RknnPool.hpp

```cpp
// put() - 锁顺序修复
int RknnPool::put(inputType inputData) {
    int modelId = getModelId();  // 移到锁外面
    std::lock_guard<std::mutex> lock(m_queueMtx);
    m_futures.push(m_pool->submit(&rknnModel::infer, m_models[modelId], inputData));
    return 0;
}

// get() - 提前释放锁
int RknnPool::get(outputType& outputData) {
    std::unique_lock<std::mutex> lock(m_queueMtx);  // 改用unique_lock
    if (m_futures.empty())
        return 1;
    auto fut = std::move(m_futures.front());
    m_futures.pop();
    lock.unlock();  // 新增：提前释放锁
    outputData = fut.get();
    return 0;
}
```
